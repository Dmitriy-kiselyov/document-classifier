# document-classifier

Это не классический README о проекте, а просто запись мыслей и план дальнийших действий.

## Этап 1. Клонирование существующей программы

На данный момент реализован алгоритм классификации, что был в моей курсовой работе, с некоторыми поправками на usability.

Алгоритм имеет такой поток выполнения:

* __Этап подготовки данных__
* Программа внутри каждой папки случайным образом разделяет документы на 2 кучи: классифицированные и те, что необходимо распознать.
Сейчас это разбиение составляет 80% и 20% соответственно;
* По всем известным документам строится словарь уникальных слов (обычный `HashMap`);
* Для каждой папки документов получаем маски этих самых папок с помощью словаря;
Маска представляет из себя (большой массив из `0` и `1`, где `0` – слова нет в словаре и `1`, если оно там есть). За позиции
слов в маске отвечает сам словарь;
* По каждой маске строится нейрон;
* __Этап классификации__
* Для каждого нераспознанного документа создается маска
* Каждая маска тестируется на всех нейронах
* Документ принадлежит тому типу нейрона, который распознает документ сильнее всего
* По этим данным собирается статистика

На данный момент качество классификации нейронов плохое:
<img src="https://i.imgur.com/deUJZc5.png" width=600/>

На это есть множество причин. Перечислю:
* Словарь содержит очень много мусорных слов, к примеру: `gfk jta vpd isc rit`.
  Это какие-то слова в шапке письма, разбитый на части адрес почты и т.д. Уровень классификации на данных "без мусора" ожидается лучше.
  Возможно, стоит фильтровать слова через словарь настоящих английских слов. А, может, совсем перейти на такой словарь и не подсчитывать
  по документам. Также частая практика: отфильтровывать слова с низкой частотой встречаний;
* Нужен стеминг (унисексация) слов [ссылка](http://snowball.tartarus.org/)
* Все слова сейчас имеют одинаковый вес. Однако, есть часто встрчающиеся слова: `in I is the...`, а есть специфичные для данного текста.
Нужно каждой группе слов задать свой вес. Так, например, предлоги будут иметь ничтожный вес, а специальные термины – огромный.
Для такой реализации нужен словарь, в котором будет указана "природа слова".
Дополнительно можно учитывать количество используемых слов и удалять редкие.
* На данный момент нейроны имеют алгоритм обучения, но при этом он не используется. Думаю, правильным решением будет
назначить нейрону случайные веса (как это и делается в обычных нейронных сетях) и уже корректировать нейрон,
обучая его на всех доступных файлах;
* Слова в документе, находящиеся рядом, имеют более тесную связь. Можно эти связи как-то учитывать (называется _биграмма_);
* __Контекст__. Диплом называется: "Классификация документов с использованием контекста". Так что нужно подумать, как его сюда прикрутить.

_Дополнительно_:
* сделать нормальное цветное логгирование событий
* высчитать сложность алгоритма и придумать способ оптимизации
* запустить паука по РИНЦ, поскачивать документы и классифицировать их по УДК (но это когда будет ну сооовсем нечем себя занять)
* продумать возможность сохранения данных нейронов

# Этап 2. Улучшение словаря

[очень полезная ссылка](https://habrahabr.ru/post/332078/)

У текущей реализации словаря есть гиганстская проблема – он содержит в себе мусорные слова. Что это значит?
Возьмем, например, начало текста из документа выборки:

```
Xref: cantaloupe.srv.cs.cmu.edu alt.atheism:49960 alt.atheism.moderated:713 news.answers:7054 alt.answers:126
Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!magnus.acs.ohio-state.edu!usenet.ins.cwru.edu!agate!spool.mu.edu!uunet!pipex!ibmpcug!mantis!mathew
From: mathew <mathew@mantis.co.uk>
Newsgroups: alt.atheism,alt.atheism.moderated,news.answers,alt.answers
```

Это заголовки письма. Конечно же, любой формат содержит в себе некие описанные определенным образом данные,
и в классификаторе при парсинге документа можно их просто проигнорировать.

Рассмотрим, что сделает текущая реализация парсинга
```js
text.trim().split(/[ \n\r,.:/"'+-=<>?!()*$_#`%&\t[\]{}\\^|~]+/g);
```
с такой строчкой:
```
cantaloupe.srv.cs.cmu.edu => [cantaloupe, srv, cs, cmu, edu]
```
То есть текущая реализация разобъет эту очевидно лишнюю для классификатора строчку на аж
целых 5 слов! и добавит в словарь. Такое загрязнение и проводит с таким низким
показателям алгоритма и раздутому (100'000 слов) словарю. Нужно не разбивать слова
бездумно по символам, а оставить разбиение по пробелам, и удалять те, что
имеют большое кол-во знаков внутри себя.

Однако, нужно реализовать это аккуратно, чтобы не нарушить _семантически верный_ парсинг текста.
То есть строчку `привет,мир` нужно разбить на слова `привет, мир`, а строчку
`dmitriy.penetrator@gmail.com` удалить полностью, не разбивая по символам.

**Решение**:

Решил разбить разделяющие символы на 2 группы: те, что всегда разбивают строку на слова:
` \n\r\t,/|\+–=<>()[]{}` и те, что могут находится только на конце слова:
`_.:-?!"'~#;*`. Первые из них использует `Reader` при разбиении строк, второй
– зона ответственности `transformers`, они удаляются только по краям.

_Почему выбраны именно такие символы?_ Символы выбраны так из двух причин:
1) Человек, набравший текст, мог по ошибке не поставить пробел между словами,
но удалять из словаря 2 слова из-за этого не хочется, поэтому я предугадал
эти ошибки, так напирмер фразу `привет,как дела` или `Дима+Таня=любовь` можно
написать без пробелов, а вот ошибку типа `hello"world"` человек вряд ли пропустит.
2) Специальные слова. _Почему `.!?` не входят в символы первой группы?_
Ответ на этот вопрос дает пример, приведенный выше:
`cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu`, `mathew@mantis.co.uk`.
Шанс, что эти символы являются частью специальных конструций гораздо выше, чем то,
что человек забыл поставить пробел. Да и если в словарь не попадут слова из
`Привет!Как дела?` будет лучше, чем если в словаре окажутся
`cantaloupe srv cs cmu edu`.
3) Также во вторую группу были включены `~#*` и подобные символы. Это задел на будущее.
Например, если в выборке окажутся тексты с языками разметки или программирования.

-------------------------------------

После нововведений размер словаря возвос до `120'000`! Однако, это совсем не плохо.
Даже наоборот, хорошо. Разница означает, что как минимум `20'000` слов в словаре мусорных!
Зато теперь слова `cantaloupe.srv.cs.cmu.edu`, `p?#$a.0rq'&`, `j;^i` четко видны в словаре, а значит пришла пора
вводить новый фильтр, который будет их убирать.

-------------------------------------

Добавил фильтр английских слов: `/^[A-Za-z]([-_'&]?[A-Za-z])*$/`. Количество
слов в словаре, правда, снова уменьшилось до 100'000, однако ожидается, что при этом
качество классификации возрастет. Тем не менее, это означает, что все-таки
необходимо сокращать редко встречающиеся слова в словаре.

_В планах_: когда появится конфиг, сделать опцию выбора языка, чтобы пользователь
мог передать кастомную функцию или regex.
